{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d629830b-2cee-4356-9472-42ffb21c57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "from minio import Minio\n",
    "from delta.tables import *\n",
    "import os\n",
    "\n",
    "def minio_session_spark():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"appMinIO\")\n",
    "            ### Config Fields\n",
    "            .config('spark.sql.debug.maxToStringFields', 5000)\n",
    "            .config('spark.debug.maxToStringFields', 5000)\n",
    "            ### Optimize\n",
    "            .config(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "            .config(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "            ### Delta Table\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            ## MinIO\n",
    "            #.config(\"spark.hadoop.fs.s3a.endpoint\", \"http://172.20.0.2:9000\")\n",
    "             .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9000\")\n",
    "\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"tcc_user\")\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"Acnmne@a9h!\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            ## Jars\n",
    "            .config(\"spark.jars\", \"/home/jovyan/work/jars/hadoop-common-3.3.2.jar,\\\n",
    "                                    /home/jovyan/work/jars/hadoop-aws-3.3.2.jar, \\\n",
    "                                    /home/jovyan/work/jars/aws-java-sdk-bundle-1.11.874.jar\")\n",
    "            .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34501eee-0c76-4ee1-b562-e75762b2223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 3.3.2\n",
      "Hadoop version = 3.3.2\n"
     ]
    }
   ],
   "source": [
    "spark = minio_session_spark()\n",
    "\n",
    "# spark\n",
    "print(f\"Spark version = {spark.version}\")\n",
    "\n",
    "# hadoop\n",
    "print(f\"Hadoop version = {spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7c018-4b15-436f-857b-877e1c8975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Minio connection\n",
    "minio_endpoint = 'minio:9000'\n",
    "access_key = 'tcc_fia'\n",
    "secret_key = 'tcc_fia'\n",
    "secure = False  # Set to True for HTTPS\n",
    "minio_client = Minio(endpoint=minio_endpoint, access_key=access_key, secret_key=secret_key, secure=secure)\n",
    "\n",
    "# Specify the Minio bucket and path\n",
    "minio_bucket = 'raw'\n",
    "minio_path_bronze_players = ['bronze_I/', 'bronze_II/','bronze_III/','bronze_IV/']\n",
    "minio_path_silver_players = ['silver_I/', 'silver_II/','silver_III/','silver_IV/']\n",
    "minio_path_ouro_players = ['gold_I/', 'gold_II/','gold_III/','gold_IV/']\n",
    "minio_path_platina_players = ['platinum_I/', 'platinum_II/','platinum_III/','platinum_IV/']\n",
    "# minio_path_esmeralda_players = ['emerald_I/', 'emerald_II/','emerald_III/','emerald_IV/']\n",
    "# minio_path_diamante_players = ['diamond_I/', 'diamond_II/','diamond_III/','diamond_IV/']\n",
    "\n",
    "\n",
    "#BRONZE PLAYERS\n",
    "for rank in minio_path_bronze_players:\n",
    "    print(f'Executando {rank}')\n",
    "    objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "    # Read JSON files into a PySpark DataFrame\n",
    "    json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "    df = spark.read.json(json_files[1:])\n",
    "    \n",
    "    #Saving Customer Segmentation in Bronze Layer\n",
    "    (\n",
    "        df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\") \n",
    "        .option(\"overwriteSchema\", \"True\")\n",
    "        .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "    )\n",
    "\n",
    "#SILVER PLAYERS\n",
    "for rank in minio_path_silver_players:\n",
    "    print(f'Executando {rank}')\n",
    "    objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "    # Read JSON files into a PySpark DataFrame\n",
    "    json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "    df = spark.read.json(json_files[1:])\n",
    "    \n",
    "    #Saving Customer Segmentation in Bronze Layer\n",
    "    (\n",
    "        df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\") \n",
    "        .option(\"overwriteSchema\", \"True\")\n",
    "        .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "    )\n",
    "    \n",
    "# #GOLD PLAYERS\n",
    "# for rank in minio_path_ouro_players:\n",
    "#     objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "#     # Read JSON files into a PySpark DataFrame\n",
    "#     json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "#     df = spark.read.json(json_files[1:])\n",
    "    \n",
    "#     #Saving Customer Segmentation in Bronze Layer\n",
    "#     (\n",
    "#         df\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\") \n",
    "#         .option(\"overwriteSchema\", \"True\")\n",
    "#         .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "#     )\n",
    "\n",
    "# #PLATINA PLAYERS\n",
    "# for rank in minio_path_platina_players:\n",
    "#     objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "#     # Read JSON files into a PySpark DataFrame\n",
    "#     json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "#     df = spark.read.json(json_files[1:])\n",
    "    \n",
    "#     #Saving Customer Segmentation in Bronze Layer\n",
    "#     (\n",
    "#         df\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\") \n",
    "#         .option(\"overwriteSchema\", \"True\")\n",
    "#         .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "#     )\n",
    "    \n",
    "# #ESMERALDA PLAYERS\n",
    "# for rank in minio_path_esmeralda_players:\n",
    "#     objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "#     # Read JSON files into a PySpark DataFrame\n",
    "#     json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "#     df = spark.read.json(json_files[1:])\n",
    "    \n",
    "#     #Saving Customer Segmentation in Bronze Layer\n",
    "#     (\n",
    "#         df\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\") \n",
    "#         .option(\"overwriteSchema\", \"True\")\n",
    "#         .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "#     )\n",
    "    \n",
    "# #DIAMANTE PLAYERS\n",
    "# for rank in minio_path_diamante_players:\n",
    "#     objects = minio_client.list_objects(minio_bucket, prefix=rank)\n",
    "\n",
    "#     # Read JSON files into a PySpark DataFrame\n",
    "#     json_files = [f\"s3a://{minio_bucket}/{obj.object_name}\" for obj in objects]\n",
    "#     df = spark.read.json(json_files[1:])\n",
    "    \n",
    "#     #Saving Customer Segmentation in Bronze Layer\n",
    "#     (\n",
    "#         df\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\") \n",
    "#         .option(\"overwriteSchema\", \"True\")\n",
    "#         .save(f\"s3a://bronze/\" + f'{rank}')\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
